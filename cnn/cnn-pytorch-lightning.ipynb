{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchmetrics\n",
    "import pytorch_lightning as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./CIFAR10/data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./CIFAR10/data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "test_dl = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ('plane', 'car', 'brid', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Input size is 3 because we will send 3 types of color channels\n",
    "input_size = 3\n",
    "output_size = 6\n",
    "kernel_size = 5\n",
    "\n",
    "class ConvNet(pl.LightningModule):\n",
    "    def __init__(self, learning_rate):\n",
    "        super().__init__()\n",
    "        self.learning_rate = learning_rate\n",
    "        self.configure_metrics()\n",
    "\n",
    "        # Feature learning\n",
    "        self.conv1 = nn.Conv2d(input_size, output_size, kernel_size)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(output_size, 16, kernel_size)\n",
    "        \n",
    "        # Classification\n",
    "        self.fc1 = nn.Linear(16*5*5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "        # self.fc1.register_forward_hook(self.activation_hook)\n",
    "        # self.fc2.register_forward_hook(self.activation_hook)\n",
    "        # self.fc3.register_forward_hook(self.activation_hook)\n",
    "        # self.get_all_layers()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.reshape(-1, 16*5*5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    # def train_dataloader(self):\n",
    "    #     return torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "    # def val_dataloader(self):\n",
    "    #     return torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "    \n",
    "    def configure_metrics(self):\n",
    "        self.train_acc = torchmetrics.Accuracy()\n",
    "        self.valid_acc = torchmetrics.Accuracy()\n",
    "        self.valid_precision = torchmetrics.Precision(num_classes=10)\n",
    "        self.valid_recall = torchmetrics.Recall(num_classes=10)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        x, y = train_batch\n",
    "        output = self(x)\n",
    "        loss = nn.CrossEntropyLoss()(output, y)\n",
    "        self.train_acc(output, y)\n",
    "        self.log('train_acc', self.train_acc, on_step=False, on_epoch=True, logger=True)\n",
    "        self.log('train_loss', loss, on_step=False, on_epoch=True, logger=True)\n",
    "        \n",
    "        return loss    \n",
    "\n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        x, y = val_batch\n",
    "        output = self(x)\n",
    "        loss = nn.CrossEntropyLoss()(output, y)\n",
    "        \n",
    "        self.valid_precision(output, y)\n",
    "        self.valid_recall(output, y)\n",
    "        self.valid_acc(output, y)\n",
    "        self.log(\"precision\", self.valid_precision, on_step=False, on_epoch=True, logger=True)\n",
    "        self.log(\"recall\", self.valid_recall, on_step=False, on_epoch=True, logger=True)\n",
    "        self.log('val_acc', self.valid_acc, on_step=False, on_epoch=True, logger=True)\n",
    "        self.log('val_loss', loss, on_step=False, on_epoch=True, logger=True)\n",
    "\n",
    "    # def activation_hook(self, inst, inp, out):\n",
    "    #     \"\"\"Run activation hook\n",
    "\n",
    "    #     Parameters\n",
    "    #     ----------\n",
    "    #     inst : torch.nn.Module\n",
    "    #         The layer we want to attach the hook to.\n",
    "    #     inp : torch.Tensor\n",
    "    #         The input to the `forward` method.\n",
    "    #     out : torch.Tensor\n",
    "    #         The output of the `forward` method.\n",
    "    #     \"\"\"\n",
    "    #     # tb = SummaryWriter()\n",
    "    #     # Create histogram of layer weights\n",
    "    #     self.logger.experiment.add_histogram(repr(inst), out)\n",
    "\n",
    "    #     img_grid = torchvision.utils.make_grid(inp[0])\n",
    "    #     self.logger.experiment.add_image('Output images', img_grid)\n",
    "\n",
    "    #     img_grid = torchvision.utils.make_grid(out)\n",
    "    #     self.logger.experiment.add_image('Output images', img_grid)\n",
    "\n",
    "model = ConvNet(learning_rate)\n",
    "\n",
    "# model.fc1.register_forward_hook(model.activation_hook)\n",
    "# model.fc2.register_forward_hook(model.activation_hook)\n",
    "# model.fc3.register_forward_hook(model.activation_hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import Callback\n",
    "\n",
    "\n",
    "class MyCallback(Callback):\n",
    "    # def on_init_start(self, trainer):\n",
    "        # trainer.lightning_module.logger.experiment\n",
    "        # self.writer = trainer.lightning_module.logger.experiment\n",
    "        # trainer.lightning_module.fc1.register_forward_hook(self.activation_hook)\n",
    "        # trainer.lightning_module.fc2.register_forward_hook(self.activation_hook)\n",
    "        # trainer.lightning_module.fc3.register_forward_hook(self.activation_hook)\n",
    "\n",
    "    def on_fit_start(self, trainer, pl_module):\n",
    "        \"\"\"Callback function that gets executed before the fit starts\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        trainer : pl.Trainer\n",
    "            The trainer of the CNN module (pl_module)\n",
    "        pl_module : pl.LightningModule\n",
    "            The model we want to use to retrieve information\n",
    "        \"\"\"\n",
    "        print(\"Starting to fit trainer!\")\n",
    "        \n",
    "        self.writer = pl_module.logger.experiment\n",
    "        pl_module.fc1.register_forward_hook(self.activation_hook)\n",
    "        pl_module.fc2.register_forward_hook(self.activation_hook)\n",
    "        pl_module.fc3.register_forward_hook(self.activation_hook)\n",
    "\n",
    "    def activation_hook(self, inst, inp, out):\n",
    "        \"\"\"Run activation hook\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        inst : torch.nn.Module\n",
    "            The layer we want to attach the hook to.\n",
    "        inp : torch.Tensor\n",
    "            The input to the `forward` method.\n",
    "        out : torch.Tensor\n",
    "            The output of the `forward` method.\n",
    "        \"\"\"\n",
    "        # Create histogram of layer weights\n",
    "        self.writer.add_histogram(repr(inst), out)\n",
    "\n",
    "        # idx = torch.randint(0, inp[0].size(0), ())\n",
    "        # pred = self.normalize_output(inp[0][idx, 0])\n",
    "\n",
    "        img_grid = torchvision.utils.make_grid(inp[0])\n",
    "        self.writer.add_image('Forward Input images', img_grid)\n",
    "\n",
    "        idx = torch.randint(0, out.size(0), ())\n",
    "        pred = self.normalize_output(out[idx, 0])\n",
    "\n",
    "        img_grid = torchvision.utils.make_grid(out)\n",
    "        self.writer.add_image('Forward Output images', img_grid)\n",
    "\n",
    "    # def get_all_layers(self, model):\n",
    "    #     \"\"\"Gets all the layers of the CNN\n",
    "\n",
    "    #     Parameters\n",
    "    #     ----------\n",
    "    #     model : pl.LightningModule\n",
    "    #         The model we want to use to retrieve information\n",
    "    #     \"\"\"\n",
    "    #     for name, layer in model._modules.items():\n",
    "    #         if isinstance(layer, nn.Sequential):\n",
    "    #             self.get_all_layers(layer)\n",
    "    #         else:\n",
    "    #             layer.register_forward_hook(self.activation_hook)\n",
    "\n",
    "    # def matplotlib_imshow(self, img, one_channel=False):\n",
    "    #     if one_channel:\n",
    "    #         img = img.mean(dim=0)\n",
    "    #     img = img / 2 + 0.5\n",
    "    #     npimg = img.numpy()\n",
    "        # if one_channel:\n",
    "        #     plt.imshow(npimg, cmap=\"Greys\")\n",
    "        # else:\n",
    "        #     plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "    # def on_train_epoch_end(self, trainer, pl_module):\n",
    "    #     print(\"Callbak training epoch end\")\n",
    "\n",
    "    #     for name, params in pl_module.named_parameters():\n",
    "    #         print(f\"Callback for in {pl_module.current_epoch}\")\n",
    "    #         pl_module.logger.experiment.add_histogram(name, params, pl_module.current_epoch)\n",
    "\n",
    "    # def on_validation_batch_start(self, trainer, pl_module, batch, batch_idx, dataloader_idx):\n",
    "    #     x, y = batch\n",
    "    #     tb = pl_module.logger.experiment\n",
    "\n",
    "    #     # img = np.reshape(x[0:], -1, 28, 28, 1)\n",
    "    #     grid = torchvision.utils.make_grid(x, normalize=True)\n",
    "    #     tb.add_image('Epoch start images', grid)\n",
    "\n",
    "    # def on_validation_batch_end(self, trainer, pl_module, outputs, batch, batch_idx, dataloader_idx):\n",
    "    #     x, y = outputs\n",
    "    #     tb = pl_module.logger.experiment\n",
    "\n",
    "    #     # img = np.reshape(x[0:], -1, 28, 28, 1)\n",
    "    #     grid = torchvision.utils.make_grid(x, normalize=True)\n",
    "    #     tb.add_image('Epoch end images', grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find best learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = pl.Trainer(auto_lr_find=True)\n",
    "# lr_finder = trainer.tuner.lr_find(model)\n",
    "# lr_finder.results\n",
    "# fig = lr_finder.plot(suggest=True)\n",
    "# fig.show()\n",
    "# new_lr = lr_finder.suggestion()\n",
    "# model.hparams.lr = new_lr\n",
    "# print(new_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "\n",
      "  | Name            | Type      | Params\n",
      "----------------------------------------------\n",
      "0 | train_acc       | Accuracy  | 0     \n",
      "1 | valid_acc       | Accuracy  | 0     \n",
      "2 | valid_precision | Precision | 0     \n",
      "3 | valid_recall    | Recall    | 0     \n",
      "4 | conv1           | Conv2d    | 456   \n",
      "5 | pool            | MaxPool2d | 0     \n",
      "6 | conv2           | Conv2d    | 2.4 K \n",
      "7 | fc1             | Linear    | 48.1 K\n",
      "8 | fc2             | Linear    | 10.2 K\n",
      "9 | fc3             | Linear    | 850   \n",
      "----------------------------------------------\n",
      "62.0 K    Trainable params\n",
      "0         Non-trainable params\n",
      "62.0 K    Total params\n",
      "0.248     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to fit trainer!\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Dev-tools\\anaconda\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:110: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Dev-tools\\anaconda\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:110: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 1876/1876 [03:45<00:00,  8.33it/s, loss=1.24, v_num=1]\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(max_epochs=2, callbacks=[MyCallback()])\n",
    "trainer.fit(model, train_dl, test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorboard` not found.\n"
     ]
    }
   ],
   "source": [
    "%tensorboard"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5d433e2eeacfaf9c6777051e9a6e8f217b0a42d6ada65163480106221da8bdb7"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
